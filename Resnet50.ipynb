{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.models as models\n",
    "root = '/Users/kaiwendai/ML-project/cropped_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform():\n",
    "    transforms = [T.Resize((256,256)),T.ToTensor(),]\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root, transform = data_transform())\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.6)\n",
    "val_size = int(dataset_size * 0.2)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "    model.train() \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        data, target = data.to(device), target.to(device)     # use gpu if possible\n",
    "        optimizer.zero_grad()                                 # clean gradients\n",
    "        output = model(data)                                  # make a prediction\n",
    "        loss = criterion(output, target)                      # what is loss\n",
    "\n",
    "        loss.backward()                                       # calculate loss\n",
    "        optimizer.step()                                      # update weights (update filters in this case)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset), #60000\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.item()\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) \n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "                       \n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2443 (0.000000%)]\tLoss: 0.574998\n",
      "Train Epoch: 1 [400/2443 (16.366612%)]\tLoss: 0.691860\n",
      "Train Epoch: 1 [800/2443 (32.733224%)]\tLoss: 0.393034\n",
      "Train Epoch: 1 [1200/2443 (49.099836%)]\tLoss: 0.661761\n",
      "Train Epoch: 1 [1600/2443 (65.466448%)]\tLoss: 0.039063\n",
      "Train Epoch: 1 [2000/2443 (81.833061%)]\tLoss: 0.131410\n",
      "Train Epoch: 1 [2400/2443 (98.199673%)]\tLoss: 0.002097\n",
      "\n",
      "Test set: Average loss: 0.3045, Accuracy: 737/815 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, device, train_loader, optimizer, epoch=1,log_interval=100)\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
