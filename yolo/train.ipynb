{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"train.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"A9rwdG2aoXpK"},"source":["**Train YOLO model**\n","\n","Original Author: Bubbliiiing\n","\n","Edit / Rewrite / Comment By: Hammond Liu (hl3797)\n","\n","Github Link: [yolov4-pytorch](https://github.com/bubbliiiing/yolov4-pytorch)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSyfWGlSCVDT","executionInfo":{"status":"ok","timestamp":1607150567026,"user_tz":-480,"elapsed":18351,"user":{"displayName":"Haoming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"0f9ffb1c-993c-4c72-d673-cf14242aa93a"},"source":["# Mount to Google Drive to train on Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9WvKiV-CWrr","executionInfo":{"status":"ok","timestamp":1607150570481,"user_tz":-480,"elapsed":2296,"user":{"displayName":"Haoming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"1fd643fd-776c-4de3-dbfc-999fe5fd9599"},"source":["# Switch to the working path of this project\n","from os import chdir, listdir\n","\n","print(listdir('./'))\n","\n","chdir('/content/drive/MyDrive/Sem_3/ML/F20_ML_final_project/yolo/')\n","\n","print(listdir('./'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['.config', 'drive', 'sample_data']\n","['utils', 'yolo.py', 'nets', 'data', 'yolo_ann.ipynb', 'train.ipynb']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x8FQkG9UoP7p"},"source":["import os\n","import time\n","import copy\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.backends.cudnn as cudnn\n","from nets.yolo4 import YoloBody\n","from nets.yolo_training import YOLOLoss, Generator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTPF_IzuoP7t"},"source":["# Read the format data info from input files\n","def get_classes(classes_path):\n","    '''loads the classes'''\n","    with open(classes_path) as f:\n","        class_names = f.readlines()\n","    class_names = [c.strip() for c in class_names]\n","    return class_names\n","\n","def get_anchors(anchors_path):\n","    '''loads the anchors from a file'''\n","    with open(anchors_path) as f:\n","        anchors = f.readline()\n","    anchors = [float(x) for x in anchors.split(',')]\n","    return np.array(anchors).reshape([-1,3,2])[::-1,:,:]\n","\n","anchors_path = './data/anchors.txt'\n","classes_path = './data/classes.txt'   \n","class_names = get_classes(classes_path)\n","anchors = get_anchors(anchors_path)\n","num_classes = len(class_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VFA-jirqOIc"},"source":["# Train one epoch\n","def fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen,genval, Epoch, cuda, optimizer, lr_scheduler):\n","    total_loss = 0\n","    val_loss = 0\n","    print('\\n' + '-' * 10 + 'Train one epoch.' + '-' * 10)\n","    print('Epoch:'+ str(epoch+1) + '/' + str(Epoch))\n","    print('Start Training.')\n","    net.train()\n","    for iteration in range(epoch_size):\n","        start_time = time.time()\n","        images, targets = next(gen)\n","        with torch.no_grad():\n","            if cuda:\n","                images = Variable(torch.from_numpy(images).type(torch.FloatTensor)).cuda()\n","                targets = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets]\n","            else:\n","                images = Variable(torch.from_numpy(images).type(torch.FloatTensor))\n","                targets = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets]\n","        optimizer.zero_grad()\n","        outputs = net(images)\n","        losses = []\n","        for i in range(3):\n","            loss_item = yolo_losses[i](outputs[i], targets)\n","            losses.append(loss_item[0])\n","        loss = sum(losses)\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        total_loss += loss\n","        waste_time = time.time() - start_time\n","        if iteration == 0 or (iteration+1) % 10 == 0:\n","            print('step:' + str(iteration+1) + '/' + str(epoch_size) + ' || Total Loss: %.4f || %.4fs/step' % (total_loss/(iteration+1), waste_time))\n","    print('Finish Training.')\n","            \n","    # print('Start Validation.')\n","    # net.eval()\n","    # for iteration in range(epoch_size_val):\n","    #     images_val, targets_val = next(genval)\n","\n","    #     with torch.no_grad():\n","    #         if cuda:\n","    #             images_val = Variable(torch.from_numpy(images_val).type(torch.FloatTensor)).cuda()\n","    #             targets_val = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets_val]\n","    #         else:\n","    #             images_val = Variable(torch.from_numpy(images_val).type(torch.FloatTensor))\n","    #             targets_val = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets_val]\n","    #         optimizer.zero_grad()\n","    #         outputs = net(images_val)\n","    #         losses = []\n","    #         for i in range(3):\n","    #             loss_item = yolo_losses[i](outputs[i], targets_val)\n","    #             losses.append(loss_item[0])\n","    #         loss = sum(losses)\n","    #         val_loss += loss\n","    # print('Finish Validation')\n","    \n","    print('Total Loss: %.4f || Val Loss: %.4f ' % (total_loss/(epoch_size+1), val_loss/(epoch_size_val+1)))\n","    \n","    return total_loss/(epoch_size+1), val_loss/(epoch_size_val+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlcqt9W0oP7w"},"source":["# Initialize variables\n","\n","input_shape = (416,416)\n","# input_shape = (608, 608)\n","\n","# Apply [cosine lr decay] and [mosaic data augment]\n","Cosine_lr = True\n","mosaic = True\n","\n","Cuda = True\n","smoooth_label = 0.03\n","\n","train_annotation_path = './data/train_data.txt'\n","val_annotation_path = './data/val_data.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvtMjzCpoP7z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607150933443,"user_tz":-480,"elapsed":3224,"user":{"displayName":"Haoming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"13a10f85-83cb-4d3a-9a6b-c0a440b16955"},"source":["# Create the model & Load pretrained weights\n","model = YoloBody(len(anchors[0]), num_classes)\n","\n","# model_path = \"./data/yolov4_coco_pretrained_weights.pth\"\n","model_path = \"./data/yolov4_maskdetect_weights1.pth\"\n","print('Loading pretrained model weights.')\n","model_dict = model.state_dict()\n","pretrained_dict = torch.load(model_path)\n","pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) ==  np.shape(v)}\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","print('Finished!')\n","\n","# Use cuda for training\n","if Cuda:\n","    net = torch.nn.DataParallel(model)\n","    cudnn.benchmark = True\n","    net = net.cuda()\n","\n","# Set up YOLO loss function\n","yolo_losses = []\n","for i in range(3):\n","    yolo_losses.append(YOLOLoss(np.reshape(anchors, [-1,2]), num_classes, \\\n","                                (input_shape[1], input_shape[0]), smoooth_label, Cuda))\n","\n","# Read train & val data\n","with open(train_annotation_path) as f:\n","    train_lines = f.readlines()\n","with open(val_annotation_path) as f:\n","    val_lines = f.readlines()\n","num_train = len(train_lines)\n","num_val = len(val_lines)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading pretrained model weights.\n","Finished!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UjOSue6toP72","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607152064002,"user_tz":-480,"elapsed":1124604,"user":{"displayName":"Haoming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"bbc01320-46e6-4186-8e44-665491c59c2e"},"source":["# Freeze the backbone and train 25 epochs\n","lr = 1e-3\n","Batch_size = 4\n","Init_Epoch = 0\n","Freeze_Epoch = 25\n","\n","# Use Adam optimization & Apply cosine annealing lr\n","optimizer = optim.Adam(net.parameters(), lr, weight_decay=5e-4)\n","if Cosine_lr:\n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n","else:\n","    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","# Apply mosaic data augment\n","gen = Generator(Batch_size, train_lines, (input_shape[0], input_shape[1])).generate(mosaic = mosaic)\n","gen_val = Generator(Batch_size, val_lines, (input_shape[0], input_shape[1])).generate(mosaic = False)\n","\n","# Compute epoch size & Set requires_grad\n","epoch_size = int(max(1, num_train // Batch_size // 2.5)) if mosaic else max(1, num_train // Batch_size)\n","epoch_size_val = num_val // Batch_size\n","for param in model.backbone.parameters():\n","    param.requires_grad = False\n","\n","# Training\n","best_loss = 99999999.0\n","best_model_weights = copy.deepcopy(net.state_dict())\n","for epoch in range(Init_Epoch, Freeze_Epoch):\n","    # Fit one epoch\n","    total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n","                                         Freeze_Epoch, Cuda, optimizer, lr_scheduler)\n","    # Update the best loss if needed\n","    if total_loss < best_loss:\n","        best_loss = total_loss\n","        best_model_weights = copy.deepcopy(model.state_dict())\n","    # Dump the loss data\n","    with open('total_loss.csv', mode='a+') as total_loss_file:\n","        total_loss_file.write(str(total_loss.item()) + '\\n')\n","    # with open('val_loss.csv', mode='a+') as val_loss_file:\n","    #     val_loss_file.write(str(val_loss.item()) + '\\n')\n","\n","# Save the weights for the first 25 epochs\n","torch.save(best_model_weights, './data/yolov4_maskdetect_weights_test.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","----------Train one epoch.----------\n","Epoch:1/25\n","Start Training.\n","step:1/46 || Total Loss: 7342.2622 || 10.6964s/step\n","step:10/46 || Total Loss: 5057.4844 || 2.9515s/step\n","step:20/46 || Total Loss: 3672.5044 || 3.0325s/step\n","step:30/46 || Total Loss: 2847.1348 || 2.3049s/step\n","step:40/46 || Total Loss: 2323.5850 || 2.4431s/step\n","Finish Training.\n","Total Loss: 2047.7594 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:2/25\n","Start Training.\n","step:1/46 || Total Loss: 514.1393 || 2.4384s/step\n","step:10/46 || Total Loss: 470.6038 || 2.8944s/step\n","step:20/46 || Total Loss: 424.4568 || 2.7061s/step\n","step:30/46 || Total Loss: 390.1014 || 2.5794s/step\n","step:40/46 || Total Loss: 361.4504 || 2.6532s/step\n","Finish Training.\n","Total Loss: 340.6440 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:3/25\n","Start Training.\n","step:1/46 || Total Loss: 223.5703 || 2.6569s/step\n","step:10/46 || Total Loss: 216.0468 || 2.5312s/step\n","step:20/46 || Total Loss: 209.2867 || 2.7817s/step\n","step:30/46 || Total Loss: 199.8475 || 0.6934s/step\n","step:40/46 || Total Loss: 190.9799 || 0.7648s/step\n","Finish Training.\n","Total Loss: 181.4763 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:4/25\n","Start Training.\n","step:1/46 || Total Loss: 144.5087 || 0.7816s/step\n","step:10/46 || Total Loss: 137.1455 || 0.7231s/step\n","step:20/46 || Total Loss: 137.9887 || 0.7291s/step\n","step:30/46 || Total Loss: 132.3648 || 0.9146s/step\n","step:40/46 || Total Loss: 127.9677 || 0.8261s/step\n","Finish Training.\n","Total Loss: 121.8879 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:5/25\n","Start Training.\n","step:1/46 || Total Loss: 86.4499 || 0.6848s/step\n","step:10/46 || Total Loss: 98.1134 || 0.8282s/step\n","step:20/46 || Total Loss: 92.2615 || 0.7471s/step\n","step:30/46 || Total Loss: 88.6981 || 0.8103s/step\n","step:40/46 || Total Loss: 89.6280 || 0.7416s/step\n","Finish Training.\n","Total Loss: 87.1907 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:6/25\n","Start Training.\n","step:1/46 || Total Loss: 82.9516 || 0.8381s/step\n","step:10/46 || Total Loss: 80.3506 || 0.8450s/step\n","step:20/46 || Total Loss: 79.0092 || 0.7448s/step\n","step:30/46 || Total Loss: 75.5236 || 0.7396s/step\n","step:40/46 || Total Loss: 71.5816 || 0.7671s/step\n","Finish Training.\n","Total Loss: 69.7442 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:7/25\n","Start Training.\n","step:1/46 || Total Loss: 54.9895 || 0.7450s/step\n","step:10/46 || Total Loss: 59.1049 || 0.6877s/step\n","step:20/46 || Total Loss: 61.6260 || 0.8454s/step\n","step:30/46 || Total Loss: 60.7950 || 0.7823s/step\n","step:40/46 || Total Loss: 59.1166 || 0.7596s/step\n","Finish Training.\n","Total Loss: 57.2327 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:8/25\n","Start Training.\n","step:1/46 || Total Loss: 69.4641 || 0.8599s/step\n","step:10/46 || Total Loss: 55.4940 || 0.7058s/step\n","step:20/46 || Total Loss: 55.7511 || 0.7666s/step\n","step:30/46 || Total Loss: 54.2310 || 0.6804s/step\n","step:40/46 || Total Loss: 52.0378 || 0.7472s/step\n","Finish Training.\n","Total Loss: 49.9577 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:9/25\n","Start Training.\n","step:1/46 || Total Loss: 43.4056 || 0.6957s/step\n","step:10/46 || Total Loss: 48.0659 || 0.7789s/step\n","step:20/46 || Total Loss: 46.9684 || 0.8323s/step\n","step:30/46 || Total Loss: 46.7068 || 0.7918s/step\n","step:40/46 || Total Loss: 45.1819 || 0.9226s/step\n","Finish Training.\n","Total Loss: 43.4833 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:10/25\n","Start Training.\n","step:1/46 || Total Loss: 35.2378 || 0.7692s/step\n","step:10/46 || Total Loss: 36.1819 || 0.7276s/step\n","step:20/46 || Total Loss: 38.5970 || 0.8794s/step\n","step:30/46 || Total Loss: 39.8593 || 0.7590s/step\n","step:40/46 || Total Loss: 39.8964 || 0.7209s/step\n","Finish Training.\n","Total Loss: 38.9764 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:11/25\n","Start Training.\n","step:1/46 || Total Loss: 29.8607 || 0.7203s/step\n","step:10/46 || Total Loss: 36.1997 || 0.9116s/step\n","step:20/46 || Total Loss: 35.2134 || 0.8051s/step\n","step:30/46 || Total Loss: 33.9739 || 0.6790s/step\n","step:40/46 || Total Loss: 34.4749 || 0.7927s/step\n","Finish Training.\n","Total Loss: 35.0948 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:12/25\n","Start Training.\n","step:1/46 || Total Loss: 35.4620 || 0.7863s/step\n","step:10/46 || Total Loss: 36.6217 || 0.7133s/step\n","step:20/46 || Total Loss: 33.9439 || 0.7236s/step\n","step:30/46 || Total Loss: 33.9858 || 0.7148s/step\n","step:40/46 || Total Loss: 33.9308 || 0.8275s/step\n","Finish Training.\n","Total Loss: 33.1286 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:13/25\n","Start Training.\n","step:1/46 || Total Loss: 28.3488 || 0.7531s/step\n","step:10/46 || Total Loss: 28.4302 || 0.7598s/step\n","step:20/46 || Total Loss: 30.3595 || 0.7190s/step\n","step:30/46 || Total Loss: 31.1995 || 0.8356s/step\n","step:40/46 || Total Loss: 31.4744 || 0.7298s/step\n","Finish Training.\n","Total Loss: 30.7373 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:14/25\n","Start Training.\n","step:1/46 || Total Loss: 37.8425 || 0.8175s/step\n","step:10/46 || Total Loss: 31.6295 || 0.6994s/step\n","step:20/46 || Total Loss: 29.6099 || 0.8144s/step\n","step:30/46 || Total Loss: 28.9053 || 0.7107s/step\n","step:40/46 || Total Loss: 28.0967 || 0.8152s/step\n","Finish Training.\n","Total Loss: 27.6011 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:15/25\n","Start Training.\n","step:1/46 || Total Loss: 21.2106 || 0.6759s/step\n","step:10/46 || Total Loss: 27.6474 || 0.8068s/step\n","step:20/46 || Total Loss: 28.7613 || 0.7861s/step\n","step:30/46 || Total Loss: 27.3800 || 0.7398s/step\n","step:40/46 || Total Loss: 27.4796 || 0.8079s/step\n","Finish Training.\n","Total Loss: 26.2606 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:16/25\n","Start Training.\n","step:1/46 || Total Loss: 20.1789 || 0.7283s/step\n","step:10/46 || Total Loss: 21.5888 || 0.6972s/step\n","step:20/46 || Total Loss: 23.2828 || 0.8308s/step\n","step:30/46 || Total Loss: 24.8197 || 0.7614s/step\n","step:40/46 || Total Loss: 23.9157 || 0.7157s/step\n","Finish Training.\n","Total Loss: 24.3488 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:17/25\n","Start Training.\n","step:1/46 || Total Loss: 28.2921 || 0.7954s/step\n","step:10/46 || Total Loss: 26.5146 || 0.8741s/step\n","step:20/46 || Total Loss: 25.8624 || 0.7182s/step\n","step:30/46 || Total Loss: 24.5198 || 0.8084s/step\n","step:40/46 || Total Loss: 23.3582 || 0.7753s/step\n","Finish Training.\n","Total Loss: 23.0508 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:18/25\n","Start Training.\n","step:1/46 || Total Loss: 25.7423 || 0.7608s/step\n","step:10/46 || Total Loss: 21.2792 || 0.7888s/step\n","step:20/46 || Total Loss: 23.0918 || 0.8178s/step\n","step:30/46 || Total Loss: 21.9016 || 0.8405s/step\n","step:40/46 || Total Loss: 23.0469 || 0.7626s/step\n","Finish Training.\n","Total Loss: 22.6363 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:19/25\n","Start Training.\n","step:1/46 || Total Loss: 33.7071 || 0.8022s/step\n","step:10/46 || Total Loss: 20.0623 || 0.7819s/step\n","step:20/46 || Total Loss: 20.2142 || 0.7632s/step\n","step:30/46 || Total Loss: 20.6651 || 0.7536s/step\n","step:40/46 || Total Loss: 19.6147 || 0.7473s/step\n","Finish Training.\n","Total Loss: 19.8597 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:20/25\n","Start Training.\n","step:1/46 || Total Loss: 14.0738 || 0.6967s/step\n","step:10/46 || Total Loss: 16.6689 || 0.7840s/step\n","step:20/46 || Total Loss: 17.5233 || 0.7192s/step\n","step:30/46 || Total Loss: 20.1728 || 0.8669s/step\n","step:40/46 || Total Loss: 20.8827 || 0.8381s/step\n","Finish Training.\n","Total Loss: 20.3618 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:21/25\n","Start Training.\n","step:1/46 || Total Loss: 20.2125 || 0.7837s/step\n","step:10/46 || Total Loss: 19.2824 || 0.7895s/step\n","step:20/46 || Total Loss: 19.6858 || 0.8205s/step\n","step:30/46 || Total Loss: 19.2264 || 0.6991s/step\n","step:40/46 || Total Loss: 19.3864 || 0.8398s/step\n","Finish Training.\n","Total Loss: 18.6480 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:22/25\n","Start Training.\n","step:1/46 || Total Loss: 22.2899 || 0.8167s/step\n","step:10/46 || Total Loss: 18.6100 || 0.6956s/step\n","step:20/46 || Total Loss: 18.4419 || 0.8274s/step\n","step:30/46 || Total Loss: 18.3791 || 0.7531s/step\n","step:40/46 || Total Loss: 18.6682 || 0.6949s/step\n","Finish Training.\n","Total Loss: 18.3702 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:23/25\n","Start Training.\n","step:1/46 || Total Loss: 26.3342 || 0.7926s/step\n","step:10/46 || Total Loss: 18.8365 || 0.7338s/step\n","step:20/46 || Total Loss: 18.9707 || 0.7598s/step\n","step:30/46 || Total Loss: 20.0057 || 0.7446s/step\n","step:40/46 || Total Loss: 19.6861 || 0.7711s/step\n","Finish Training.\n","Total Loss: 19.0674 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:24/25\n","Start Training.\n","step:1/46 || Total Loss: 30.2847 || 0.7912s/step\n","step:10/46 || Total Loss: 22.1591 || 0.8151s/step\n","step:20/46 || Total Loss: 19.5388 || 0.6986s/step\n","step:30/46 || Total Loss: 18.8828 || 0.7691s/step\n","step:40/46 || Total Loss: 19.9674 || 0.8229s/step\n","Finish Training.\n","Total Loss: 19.3811 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:25/25\n","Start Training.\n","step:1/46 || Total Loss: 15.0368 || 0.7030s/step\n","step:10/46 || Total Loss: 20.2848 || 0.8435s/step\n","step:20/46 || Total Loss: 19.0552 || 0.7747s/step\n","step:30/46 || Total Loss: 18.3337 || 0.7648s/step\n","step:40/46 || Total Loss: 18.0246 || 0.7501s/step\n","Finish Training.\n","Total Loss: 17.3593 || Val Loss: 0.0000 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FI3zgJdloP74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607153379949,"user_tz":-480,"elapsed":904113,"user":{"displayName":"Haoming Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"e2f979af-e3d1-4a8a-8270-83111206b258"},"source":["# Unfreeze the backbone and train another 25 epochs\n","lr = 1e-4\n","Batch_size = 2\n","Freeze_Epoch = 25\n","Unfreeze_Epoch = 50\n","\n","# Use Adam optimization & Apply cosine annealing lr\n","optimizer = optim.Adam(net.parameters(), lr, weight_decay=5e-4)\n","if Cosine_lr:\n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n","else:\n","    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","# Apply mosaic data augment\n","gen = Generator(Batch_size, train_lines, (input_shape[0], input_shape[1])).generate(mosaic = mosaic)\n","gen_val = Generator(Batch_size, val_lines, (input_shape[0], input_shape[1])).generate(mosaic = False)\n","\n","# Compute epoch size & Set requires_grad    \n","epoch_size = int(max(1, num_train//Batch_size//2.5)) if mosaic else max(1, num_train//Batch_size)\n","epoch_size_val = num_val//Batch_size\n","for param in model.backbone.parameters():\n","    param.requires_grad = True\n","\n","# Training\n","for epoch in range(Freeze_Epoch, Unfreeze_Epoch):\n","    # Fit one epoch\n","    total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n","                                         Unfreeze_Epoch, Cuda, optimizer, lr_scheduler)\n","    # Update the best loss (if needed)\n","    if total_loss < best_loss:\n","        best_loss = total_loss\n","        best_model_weights = copy.deepcopy(model.state_dict())\n","    # Dump the loss data\n","    with open('total_loss.csv', mode='a+') as total_loss_file:\n","        total_loss_file.write(str(total_loss.item()) + '\\n')\n","    # with open('val_loss.csv', mode='a+') as val_loss_file:\n","    #     val_loss_file.write(str(val_loss.item() + '\\n')\n","\n","# Save the final weights\n","torch.save(best_model_weights, './data/yolov4_maskdetect_weights_test_.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","----------Train one epoch.----------\n","Epoch:26/50\n","Start Training.\n","step:1/94 || Total Loss: 40.2553 || 0.6579s/step\n","step:10/94 || Total Loss: 17.9560 || 0.5680s/step\n","step:20/94 || Total Loss: 18.6807 || 0.4828s/step\n","step:30/94 || Total Loss: 20.3428 || 0.4913s/step\n","step:40/94 || Total Loss: 18.8505 || 0.5542s/step\n","step:50/94 || Total Loss: 17.7775 || 0.5225s/step\n","step:60/94 || Total Loss: 16.9284 || 0.4939s/step\n","step:70/94 || Total Loss: 16.8763 || 0.5276s/step\n","step:80/94 || Total Loss: 16.6902 || 0.4967s/step\n","step:90/94 || Total Loss: 16.2073 || 0.5372s/step\n","Finish Training.\n","Total Loss: 15.8286 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:27/50\n","Start Training.\n","step:1/94 || Total Loss: 12.2534 || 0.5120s/step\n","step:10/94 || Total Loss: 11.4258 || 0.5160s/step\n","step:20/94 || Total Loss: 13.4062 || 0.5503s/step\n","step:30/94 || Total Loss: 14.2555 || 0.5935s/step\n","step:40/94 || Total Loss: 13.8369 || 0.4738s/step\n","step:50/94 || Total Loss: 13.4154 || 0.4857s/step\n","step:60/94 || Total Loss: 13.8159 || 0.5239s/step\n","step:70/94 || Total Loss: 13.4855 || 0.4851s/step\n","step:80/94 || Total Loss: 13.5382 || 0.4911s/step\n","step:90/94 || Total Loss: 13.6679 || 0.5767s/step\n","Finish Training.\n","Total Loss: 13.5973 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:28/50\n","Start Training.\n","step:1/94 || Total Loss: 14.5247 || 0.5364s/step\n","step:10/94 || Total Loss: 10.8260 || 0.5264s/step\n","step:20/94 || Total Loss: 11.8709 || 0.4929s/step\n","step:30/94 || Total Loss: 11.7109 || 0.5354s/step\n","step:40/94 || Total Loss: 11.9676 || 0.5328s/step\n","step:50/94 || Total Loss: 11.8456 || 0.5678s/step\n","step:60/94 || Total Loss: 11.3864 || 0.5055s/step\n","step:70/94 || Total Loss: 12.0511 || 0.5078s/step\n","step:80/94 || Total Loss: 11.7259 || 0.4896s/step\n","step:90/94 || Total Loss: 11.5338 || 0.5641s/step\n","Finish Training.\n","Total Loss: 11.4985 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:29/50\n","Start Training.\n","step:1/94 || Total Loss: 17.7417 || 0.5310s/step\n","step:10/94 || Total Loss: 8.3980 || 0.4840s/step\n","step:20/94 || Total Loss: 14.3439 || 0.5113s/step\n","step:30/94 || Total Loss: 13.0521 || 0.4858s/step\n","step:40/94 || Total Loss: 12.7704 || 0.5134s/step\n","step:50/94 || Total Loss: 12.1573 || 0.5053s/step\n","step:60/94 || Total Loss: 12.2584 || 0.5771s/step\n","step:70/94 || Total Loss: 11.8444 || 0.5449s/step\n","step:80/94 || Total Loss: 11.8027 || 0.4620s/step\n","step:90/94 || Total Loss: 11.5961 || 0.5234s/step\n","Finish Training.\n","Total Loss: 11.6698 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:30/50\n","Start Training.\n","step:1/94 || Total Loss: 8.1902 || 0.4983s/step\n","step:10/94 || Total Loss: 11.2702 || 0.5016s/step\n","step:20/94 || Total Loss: 12.0456 || 0.4661s/step\n","step:30/94 || Total Loss: 12.0716 || 0.5301s/step\n","step:40/94 || Total Loss: 13.5539 || 0.4884s/step\n","step:50/94 || Total Loss: 13.5706 || 0.5120s/step\n","step:60/94 || Total Loss: 12.5573 || 0.5311s/step\n","step:70/94 || Total Loss: 11.7467 || 0.5403s/step\n","step:80/94 || Total Loss: 11.6967 || 0.5458s/step\n","step:90/94 || Total Loss: 11.7624 || 0.3763s/step\n","Finish Training.\n","Total Loss: 11.3409 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:31/50\n","Start Training.\n","step:1/94 || Total Loss: 9.3568 || 0.4840s/step\n","step:10/94 || Total Loss: 13.3689 || 0.5523s/step\n","step:20/94 || Total Loss: 16.6873 || 0.4705s/step\n","step:30/94 || Total Loss: 13.9693 || 0.5114s/step\n","step:40/94 || Total Loss: 12.5112 || 0.5507s/step\n","step:50/94 || Total Loss: 11.4913 || 0.4975s/step\n","step:60/94 || Total Loss: 10.3919 || 0.4563s/step\n","step:70/94 || Total Loss: 9.9723 || 0.5122s/step\n","step:80/94 || Total Loss: 10.0092 || 0.5251s/step\n","step:90/94 || Total Loss: 10.5814 || 0.5642s/step\n","Finish Training.\n","Total Loss: 10.8735 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:32/50\n","Start Training.\n","step:1/94 || Total Loss: 9.8399 || 0.5120s/step\n","step:10/94 || Total Loss: 12.8935 || 0.4999s/step\n","step:20/94 || Total Loss: 12.3554 || 0.4606s/step\n","step:30/94 || Total Loss: 13.1238 || 0.5147s/step\n","step:40/94 || Total Loss: 11.3453 || 0.5343s/step\n","step:50/94 || Total Loss: 10.5710 || 0.5230s/step\n","step:60/94 || Total Loss: 10.7249 || 0.5289s/step\n","step:70/94 || Total Loss: 10.9590 || 0.4743s/step\n","step:80/94 || Total Loss: 10.5343 || 0.4759s/step\n","step:90/94 || Total Loss: 11.1242 || 0.4957s/step\n","Finish Training.\n","Total Loss: 10.8155 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:33/50\n","Start Training.\n","step:1/94 || Total Loss: 18.2566 || 0.5394s/step\n","step:10/94 || Total Loss: 13.6628 || 0.5304s/step\n","step:20/94 || Total Loss: 10.1893 || 0.4630s/step\n","step:30/94 || Total Loss: 9.3770 || 0.4882s/step\n","step:40/94 || Total Loss: 10.4975 || 0.5355s/step\n","step:50/94 || Total Loss: 10.4923 || 0.4847s/step\n","step:60/94 || Total Loss: 10.9982 || 0.5593s/step\n","step:70/94 || Total Loss: 10.5389 || 0.4750s/step\n","step:80/94 || Total Loss: 10.5340 || 0.5254s/step\n","step:90/94 || Total Loss: 10.2388 || 0.5027s/step\n","Finish Training.\n","Total Loss: 10.1052 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:34/50\n","Start Training.\n","step:1/94 || Total Loss: 4.9257 || 0.4945s/step\n","step:10/94 || Total Loss: 6.9330 || 0.5005s/step\n","step:20/94 || Total Loss: 7.6534 || 0.5092s/step\n","step:30/94 || Total Loss: 9.3783 || 0.5789s/step\n","step:40/94 || Total Loss: 9.8954 || 0.4856s/step\n","step:50/94 || Total Loss: 10.7224 || 0.4986s/step\n","step:60/94 || Total Loss: 10.5558 || 0.4615s/step\n","step:70/94 || Total Loss: 10.5409 || 0.4725s/step\n","step:80/94 || Total Loss: 10.5242 || 0.5082s/step\n","step:90/94 || Total Loss: 10.1624 || 0.4651s/step\n","Finish Training.\n","Total Loss: 9.9420 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:35/50\n","Start Training.\n","step:1/94 || Total Loss: 2.0181 || 0.4733s/step\n","step:10/94 || Total Loss: 13.7026 || 0.5462s/step\n","step:20/94 || Total Loss: 12.8249 || 0.4976s/step\n","step:30/94 || Total Loss: 12.5489 || 0.5980s/step\n","step:40/94 || Total Loss: 13.3754 || 0.5068s/step\n","step:50/94 || Total Loss: 12.4498 || 0.5139s/step\n","step:60/94 || Total Loss: 11.3384 || 0.4989s/step\n","step:70/94 || Total Loss: 11.1504 || 0.5114s/step\n","step:80/94 || Total Loss: 11.5674 || 0.5581s/step\n","step:90/94 || Total Loss: 11.4948 || 0.5864s/step\n","Finish Training.\n","Total Loss: 11.3161 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:36/50\n","Start Training.\n","step:1/94 || Total Loss: 9.4491 || 0.5197s/step\n","step:10/94 || Total Loss: 12.7916 || 0.5499s/step\n","step:20/94 || Total Loss: 11.1174 || 0.4670s/step\n","step:30/94 || Total Loss: 11.0857 || 0.5280s/step\n","step:40/94 || Total Loss: 12.0247 || 0.5422s/step\n","step:50/94 || Total Loss: 11.5296 || 0.5385s/step\n","step:60/94 || Total Loss: 11.4950 || 0.5226s/step\n","step:70/94 || Total Loss: 12.2754 || 0.5519s/step\n","step:80/94 || Total Loss: 11.7656 || 0.4915s/step\n","step:90/94 || Total Loss: 11.1761 || 0.5173s/step\n","Finish Training.\n","Total Loss: 11.1024 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:37/50\n","Start Training.\n","step:1/94 || Total Loss: 14.3172 || 0.5808s/step\n","step:10/94 || Total Loss: 13.3602 || 0.6280s/step\n","step:20/94 || Total Loss: 13.2358 || 0.4880s/step\n","step:30/94 || Total Loss: 13.2969 || 0.4778s/step\n","step:40/94 || Total Loss: 11.6702 || 0.4753s/step\n","step:50/94 || Total Loss: 11.6111 || 0.5725s/step\n","step:60/94 || Total Loss: 10.8770 || 0.5040s/step\n","step:70/94 || Total Loss: 10.1484 || 0.5108s/step\n","step:80/94 || Total Loss: 10.0755 || 0.5049s/step\n","step:90/94 || Total Loss: 9.9569 || 0.5256s/step\n","Finish Training.\n","Total Loss: 9.8392 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:38/50\n","Start Training.\n","step:1/94 || Total Loss: 12.3832 || 0.5222s/step\n","step:10/94 || Total Loss: 7.8204 || 0.5049s/step\n","step:20/94 || Total Loss: 6.8354 || 0.4692s/step\n","step:30/94 || Total Loss: 8.7757 || 0.5501s/step\n","step:40/94 || Total Loss: 8.8125 || 0.3668s/step\n","step:50/94 || Total Loss: 8.5169 || 0.5116s/step\n","step:60/94 || Total Loss: 9.2567 || 0.5332s/step\n","step:70/94 || Total Loss: 9.5604 || 0.5802s/step\n","step:80/94 || Total Loss: 9.6926 || 0.5057s/step\n","step:90/94 || Total Loss: 9.8562 || 0.5449s/step\n","Finish Training.\n","Total Loss: 9.6904 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:39/50\n","Start Training.\n","step:1/94 || Total Loss: 5.7966 || 0.5259s/step\n","step:10/94 || Total Loss: 10.0012 || 0.5075s/step\n","step:20/94 || Total Loss: 9.3362 || 0.5456s/step\n","step:30/94 || Total Loss: 8.8663 || 0.5390s/step\n","step:40/94 || Total Loss: 8.2089 || 0.5373s/step\n","step:50/94 || Total Loss: 8.8172 || 0.5203s/step\n","step:60/94 || Total Loss: 8.6964 || 0.5179s/step\n","step:70/94 || Total Loss: 8.4424 || 0.5343s/step\n","step:80/94 || Total Loss: 8.4535 || 0.5047s/step\n","step:90/94 || Total Loss: 8.6471 || 0.4758s/step\n","Finish Training.\n","Total Loss: 8.6054 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:40/50\n","Start Training.\n","step:1/94 || Total Loss: 4.8356 || 0.5191s/step\n","step:10/94 || Total Loss: 13.2281 || 0.5172s/step\n","step:20/94 || Total Loss: 12.1945 || 0.5010s/step\n","step:30/94 || Total Loss: 12.1707 || 0.5286s/step\n","step:40/94 || Total Loss: 11.9282 || 0.5395s/step\n","step:50/94 || Total Loss: 12.0814 || 0.5099s/step\n","step:60/94 || Total Loss: 12.1809 || 0.5265s/step\n","step:70/94 || Total Loss: 11.7390 || 0.5435s/step\n","step:80/94 || Total Loss: 11.2405 || 0.4962s/step\n","step:90/94 || Total Loss: 10.6571 || 0.5329s/step\n","Finish Training.\n","Total Loss: 10.6783 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:41/50\n","Start Training.\n","step:1/94 || Total Loss: 9.2885 || 0.5713s/step\n","step:10/94 || Total Loss: 7.9593 || 0.5266s/step\n","step:20/94 || Total Loss: 9.4182 || 0.7632s/step\n","step:30/94 || Total Loss: 11.8117 || 0.5628s/step\n","step:40/94 || Total Loss: 11.6976 || 0.5863s/step\n","step:50/94 || Total Loss: 13.0058 || 0.5261s/step\n","step:60/94 || Total Loss: 12.7460 || 0.5367s/step\n","step:70/94 || Total Loss: 12.0403 || 0.5310s/step\n","step:80/94 || Total Loss: 11.8427 || 0.4957s/step\n","step:90/94 || Total Loss: 11.4133 || 0.4871s/step\n","Finish Training.\n","Total Loss: 11.1382 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:42/50\n","Start Training.\n","step:1/94 || Total Loss: 41.7416 || 0.5937s/step\n","step:10/94 || Total Loss: 12.0558 || 0.4902s/step\n","step:20/94 || Total Loss: 9.7631 || 0.5617s/step\n","step:30/94 || Total Loss: 9.8938 || 0.5124s/step\n","step:40/94 || Total Loss: 9.6236 || 0.5559s/step\n","step:50/94 || Total Loss: 9.1924 || 0.4901s/step\n","step:60/94 || Total Loss: 9.1253 || 0.5509s/step\n","step:70/94 || Total Loss: 9.1880 || 0.4955s/step\n","step:80/94 || Total Loss: 9.1958 || 0.5543s/step\n","step:90/94 || Total Loss: 9.1248 || 0.5641s/step\n","Finish Training.\n","Total Loss: 8.9586 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:43/50\n","Start Training.\n","step:1/94 || Total Loss: 3.7915 || 0.5255s/step\n","step:10/94 || Total Loss: 4.7030 || 0.4743s/step\n","step:20/94 || Total Loss: 7.2737 || 0.4923s/step\n","step:30/94 || Total Loss: 8.0379 || 0.5272s/step\n","step:40/94 || Total Loss: 9.0175 || 0.5370s/step\n","step:50/94 || Total Loss: 9.9390 || 0.5288s/step\n","step:60/94 || Total Loss: 9.4581 || 0.7067s/step\n","step:70/94 || Total Loss: 8.7872 || 0.4773s/step\n","step:80/94 || Total Loss: 8.7166 || 0.4713s/step\n","step:90/94 || Total Loss: 9.0353 || 0.5433s/step\n","Finish Training.\n","Total Loss: 9.1018 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:44/50\n","Start Training.\n","step:1/94 || Total Loss: 8.2734 || 0.5447s/step\n","step:10/94 || Total Loss: 7.5996 || 0.5890s/step\n","step:20/94 || Total Loss: 9.4438 || 0.5687s/step\n","step:30/94 || Total Loss: 9.3683 || 0.5131s/step\n","step:40/94 || Total Loss: 8.8009 || 0.5048s/step\n","step:50/94 || Total Loss: 8.0695 || 0.5469s/step\n","step:60/94 || Total Loss: 7.9254 || 0.5227s/step\n","step:70/94 || Total Loss: 8.8209 || 0.5897s/step\n","step:80/94 || Total Loss: 8.9541 || 0.5466s/step\n","step:90/94 || Total Loss: 9.1308 || 0.5525s/step\n","Finish Training.\n","Total Loss: 9.0859 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:45/50\n","Start Training.\n","step:1/94 || Total Loss: 22.7169 || 0.6000s/step\n","step:10/94 || Total Loss: 13.7007 || 0.5341s/step\n","step:20/94 || Total Loss: 11.6852 || 0.5280s/step\n","step:30/94 || Total Loss: 11.7995 || 0.5621s/step\n","step:40/94 || Total Loss: 10.9814 || 0.5269s/step\n","step:50/94 || Total Loss: 10.4094 || 0.5016s/step\n","step:60/94 || Total Loss: 9.8493 || 0.5358s/step\n","step:70/94 || Total Loss: 9.4028 || 0.5351s/step\n","step:80/94 || Total Loss: 9.2038 || 0.5085s/step\n","step:90/94 || Total Loss: 9.4014 || 0.5234s/step\n","Finish Training.\n","Total Loss: 9.1266 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:46/50\n","Start Training.\n","step:1/94 || Total Loss: 6.3553 || 0.5032s/step\n","step:10/94 || Total Loss: 6.8452 || 0.5177s/step\n","step:20/94 || Total Loss: 8.7650 || 0.4978s/step\n","step:30/94 || Total Loss: 9.5375 || 0.5583s/step\n","step:40/94 || Total Loss: 10.2561 || 0.5213s/step\n","step:50/94 || Total Loss: 10.0862 || 0.5244s/step\n","step:60/94 || Total Loss: 10.3140 || 0.5618s/step\n","step:70/94 || Total Loss: 9.6608 || 0.5099s/step\n","step:80/94 || Total Loss: 9.4596 || 0.5308s/step\n","step:90/94 || Total Loss: 9.1849 || 0.5157s/step\n","Finish Training.\n","Total Loss: 8.9728 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:47/50\n","Start Training.\n","step:1/94 || Total Loss: 8.0716 || 0.5428s/step\n","step:10/94 || Total Loss: 9.6526 || 0.4867s/step\n","step:20/94 || Total Loss: 8.4527 || 0.4834s/step\n","step:30/94 || Total Loss: 9.6746 || 0.5064s/step\n","step:40/94 || Total Loss: 9.3614 || 0.4744s/step\n","step:50/94 || Total Loss: 9.5723 || 0.6170s/step\n","step:60/94 || Total Loss: 9.6700 || 0.5621s/step\n","step:70/94 || Total Loss: 9.8786 || 0.5614s/step\n","step:80/94 || Total Loss: 9.4920 || 0.5094s/step\n","step:90/94 || Total Loss: 9.1356 || 0.4819s/step\n","Finish Training.\n","Total Loss: 8.8672 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:48/50\n","Start Training.\n","step:1/94 || Total Loss: 3.4033 || 0.5299s/step\n","step:10/94 || Total Loss: 8.4650 || 0.5495s/step\n","step:20/94 || Total Loss: 8.5548 || 0.5387s/step\n","step:30/94 || Total Loss: 9.4531 || 0.5254s/step\n","step:40/94 || Total Loss: 9.2811 || 0.5554s/step\n","step:50/94 || Total Loss: 8.5295 || 0.5345s/step\n","step:60/94 || Total Loss: 8.7176 || 0.5584s/step\n","step:70/94 || Total Loss: 8.6293 || 0.5535s/step\n","step:80/94 || Total Loss: 8.2789 || 0.5246s/step\n","step:90/94 || Total Loss: 8.3757 || 0.5487s/step\n","Finish Training.\n","Total Loss: 8.2709 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:49/50\n","Start Training.\n","step:1/94 || Total Loss: 4.9604 || 0.5671s/step\n","step:10/94 || Total Loss: 7.3389 || 0.5014s/step\n","step:20/94 || Total Loss: 8.5519 || 0.8549s/step\n","step:30/94 || Total Loss: 8.6768 || 0.5235s/step\n","step:40/94 || Total Loss: 9.1098 || 0.5941s/step\n","step:50/94 || Total Loss: 8.8271 || 0.4824s/step\n","step:60/94 || Total Loss: 8.5332 || 0.5410s/step\n","step:70/94 || Total Loss: 8.9415 || 0.5227s/step\n","step:80/94 || Total Loss: 8.8188 || 0.5379s/step\n","step:90/94 || Total Loss: 9.0297 || 0.5870s/step\n","Finish Training.\n","Total Loss: 8.9241 || Val Loss: 0.0000 \n","\n","----------Train one epoch.----------\n","Epoch:50/50\n","Start Training.\n","step:1/94 || Total Loss: 4.5073 || 0.5323s/step\n","step:10/94 || Total Loss: 12.7870 || 0.5256s/step\n","step:20/94 || Total Loss: 12.7772 || 0.5013s/step\n","step:30/94 || Total Loss: 11.6713 || 0.7065s/step\n","step:40/94 || Total Loss: 11.2765 || 0.5460s/step\n","step:50/94 || Total Loss: 10.7826 || 0.5339s/step\n","step:60/94 || Total Loss: 10.1378 || 0.5438s/step\n","step:70/94 || Total Loss: 9.9353 || 0.5515s/step\n","step:80/94 || Total Loss: 10.1650 || 0.5290s/step\n","step:90/94 || Total Loss: 10.3103 || 0.5388s/step\n","Finish Training.\n","Total Loss: 10.0538 || Val Loss: 0.0000 \n"],"name":"stdout"}]}]}